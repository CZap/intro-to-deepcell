{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "First, set up the environment by installing the required packages and importing the necessary functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# pip install keras numpy tensorflow sklearn scipy\n",
    "# conda install keras numpy tensorflow sklearn scipy\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare the data\n",
    "\n",
    "For this introduction, the classic MNIST dataset of hand-drawn digits is used.  Download the data, reshape it if necessary, and normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28)\n",
      "y_train shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# load the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "y_train shape: (60000,)\n",
      "First 10 values of y_train: [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "# Reshape data if necessary\n",
    "\n",
    "# \"channels_first\" is a vestige from theano, but good habit to check\n",
    "# saving input_shape to be passed to first layer of neural networks (later)\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols) # (batches, channels, rows, cols)\n",
    "else:\n",
    "    # shape = \n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1) # (batches, rows, cols, channels)\n",
    "\n",
    "# normalize the data: set pixel values to be in range [0, 1]\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('First 10 values of y_train:', y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier\n",
    "\n",
    "First, try using a linear classifier with the MNIST data.  Below is an implmentation of a Support Vector Machine from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       980\n",
      "          1       0.96      0.98      0.97      1135\n",
      "          2       0.93      0.88      0.91      1032\n",
      "          3       0.90      0.91      0.90      1010\n",
      "          4       0.92      0.93      0.92       982\n",
      "          5       0.89      0.86      0.87       892\n",
      "          6       0.93      0.95      0.94       958\n",
      "          7       0.92      0.92      0.92      1028\n",
      "          8       0.87      0.87      0.87       974\n",
      "          9       0.90      0.89      0.90      1009\n",
      "\n",
      "avg / total       0.92      0.92      0.92     10000\n",
      "\n",
      "Accuracy Score: 0.9181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Our current shape is (batches, rows, cols, channels) but SVC expects (batches, pixels)\n",
    "X_train_reshaped = np.reshape(X_train, (X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
    "X_test_reshaped = np.reshape(X_test, (X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "model.fit(X_train_reshaped, y_train)\n",
    "\n",
    "predictions = model.predict(X_test_reshaped)\n",
    "print(classification_report(y_test, predictions))\n",
    "print('Accuracy Score:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "Let's compare our linear classifier results with simple neural networks using Keras.\n",
    "\n",
    "First, convert the class vectors to binary class matrices or \"one-hot encode\" the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (60000, 10)\n",
      "y_test shape: (10000, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Using the keras to_categorical function to one-hot encode the data\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Network\n",
    "\n",
    "First, a basic one-layer example. *(Using the functional model API)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.5918 - acc: 0.8451 - val_loss: 0.3811 - val_acc: 0.8988\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.3751 - acc: 0.8966 - val_loss: 0.3379 - val_acc: 0.9069\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.3436 - acc: 0.9043 - val_loss: 0.3176 - val_acc: 0.9136\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.3268 - acc: 0.9090 - val_loss: 0.3073 - val_acc: 0.9162\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.3161 - acc: 0.9117 - val_loss: 0.3008 - val_acc: 0.9164\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=input_shape)\n",
    "model = Sequential()\n",
    "flat = Flatten()(inputs)\n",
    "dense = Dense(10, activation='softmax')(flat) # output is softmax(dot(X, W) + bias)\n",
    "model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(momentum=0.9, nesterov=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "loss = model.fit(X_train, y_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=5,\n",
    "                 verbose=1,\n",
    "                 validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Network\n",
    "\n",
    "5 epochs yield results comparable to the linear classifier, but the neural network can definitely be improved.  Below is a common pattern of Convolutions, Pooling, Dropout, and Dense layers.  [Read more about these layer types on Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network#Design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.3631 - acc: 0.8898 - val_loss: 0.1157 - val_acc: 0.9667\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0973 - acc: 0.9708 - val_loss: 0.0591 - val_acc: 0.9807\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0637 - acc: 0.9803 - val_loss: 0.0455 - val_acc: 0.9848\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0497 - acc: 0.9845 - val_loss: 0.0370 - val_acc: 0.9867\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0398 - acc: 0.9877 - val_loss: 0.0332 - val_acc: 0.9885\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 32 3x3 filters (extracting 5x5-pixel subregions), with ReLU activation\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))# Apply max filter to every 2x2 patch\n",
    "model.add(Dropout(0.25)) # Drop 25% of inputs to prevent overfitting\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(momentum=0.9, nesterov=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "loss = model.fit(X_train, y_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=5,\n",
    "                 verbose=1,\n",
    "                 validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
